{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Examples On ML\n",
    "\n",
    "In this notebook we show you some code snippets. These examples are not by any means exaustive, they are only some commons code templates for Machine Learning tasks.\n",
    "\n",
    "Almost all ML algorithms presented here are implemented by SciKitLearn library.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`Author: Flávio Clésio (flavio.clésio@movile.com)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Input/Output and Data Manipulation\n",
    "\n",
    "The most used library to manipulate data in Python is Pandas. We use Pandas to read the data and to do some basic filter/data transformation. Along with Pandas, we use Numpy, a package for scientific computation focused in matrices, and we use MatPlot (with the seaborn wrapper) lib to plot some useful graphs and give us some insights about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np # Linear algebra\n",
    "import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns #Data Visualization\n",
    "import matplotlib.pyplot as plt #Data visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path where the files are stored\n",
    "PATH = '/Volumes/PANZER/Github/ml-lab/ICPC-UNICAMP-2018/Notebooks/' #TODO: Change it to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Volumes/PANZER/Github/ml-lab/ICPC-UNICAMP-2018/Notebooks/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4e9afc2c6074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now we'll load the data (train and test data) using the read_csv() method of Pandas (pd) and parsing the dates to timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Volumes/PANZER/Github/ml-lab/ICPC-UNICAMP-2018/Notebooks/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Now we'll load the data (train and test data) using the read_csv() method of Pandas (pd) and parsing the dates to timestamp\n",
    "train = pd.read_csv(PATH + \"train.csv\", parse_dates = ['timestamp'])\n",
    "test = pd.read_csv(PATH + \"test.csv\", parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we need to transform all our data for feature engineering, \n",
    "# let's join our data to perform the same transformations for all\n",
    "\n",
    "num_train = len(train)\n",
    "num_test = len(test)\n",
    "print '# Samples in Train Dataframe2:', num_train\n",
    "print '# Samples in Test Dataframe:', num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll store the information about the Y (the variable that we need to predict) and the id's to check\n",
    "# if our prediction are correct\n",
    "\n",
    "# Store the dependent variable to a object called y_price_doc_log_train and transform to log\n",
    "Y = train['price_doc'].values\n",
    "\n",
    "# Store the id of test dataframe\n",
    "id_test = test['id']\n",
    "\n",
    "# Remove the ids (no predictive power) in both datasets and the price_doc variable from X dataset\n",
    "train.drop(['id','price_doc' ], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We bind our dataframes in a single dataframe\n",
    "df_all = pd.concat([train, test])\n",
    "\n",
    "num_df_all = len(df_all)\n",
    "\n",
    "print '# Samples in Test Dataframe:', num_df_all\n",
    "\n",
    "print 'Shape of our dataset (#Records, #Columns)', (df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To see the columns of our dataset lets use the function below\n",
    "list(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, let's take a look over the dataset \n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To see some descriptive statistics lets use the function describe()\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "We show you two basic plot. Plot the data is a very important step in Machine learning, you can get some really useful insights about the data.\n",
    "\n",
    "You can find a much more graphs examples here: \n",
    "https://seaborn.pydata.org/examples/index.html\n",
    "\n",
    "References: \n",
    "- https://matplotlib.org/gallery/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For plotting we'll use the library called Seaborn \n",
    "plt.figure(figsize=(10, 5)) # The size of the plot\n",
    "sns.distplot(Y, kde = False) # We'll use distribution plot and as first arg we use the column .price_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we can see there's a lot of outliers and to smooth this records, let's use np.log() function and see the distribution\n",
    "plt.figure(figsize=(10, 5)) # The size of the plot\n",
    "sns.distplot(np.log(Y), kde = False) # We'll use distribution plot and as first arg we use the column .price_doc\n",
    "plt.xlabel('np.log(price_doc)', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One thing can help the convergence of ML algorithms it's to remove outliers or smooth or data, to do that\n",
    "# Let's convert out Y dataset in .log\n",
    "\n",
    "Y = np.log(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "As explained in the comments bellow, we need be carefull about NaN values. Also, for some problems, it is a good ideia combine some features to create new ones. Therefore, you should know do some basic data manipulations.Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To put another variable inside the dataset. This variable will call price_sq\n",
    "df_all[\"kitch_proportions\"] = df_all[\"kitch_sq\"]/df_all[\"full_sq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When we deal with machine learning algorithms, Null values (or NaN) are trouble. You can choose \n",
    "# 1) remove then from dataset, 2) replace their values.\n",
    "# It's much better make replacemens of this values using the mean, or median or some spare value (e.g. -99999) to avoid\n",
    "# lost some predictive power. Let's check the columns with null.\n",
    "\n",
    "for col in df_all.columns.values: # For each col the loop will take the value\n",
    "    if len(df_all[df_all[col].isnull()][col]) > 0: # If the value are null, count each unit of the column\n",
    "        print(\"{0}: {1}\".format(col, len(df_all[df_all[col].isnull()][col]))) # and print the name of the column and the number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To delete some column, just use the del function\n",
    "del df_all['timestamp']\n",
    "del df_all['life_sq']\n",
    "del df_all['floor']\n",
    "del df_all['max_floor']\n",
    "del df_all['material']\n",
    "del df_all['build_year']\n",
    "del df_all['num_room']\n",
    "del df_all['kitch_sq']\n",
    "del df_all['state']\n",
    "del df_all['preschool_quota']\n",
    "del df_all['school_quota']\n",
    "del df_all['hospital_beds_raion']\n",
    "del df_all['raion_build_count_with_material_info']\n",
    "del df_all['build_count_block']\n",
    "del df_all['build_count_wood']\n",
    "del df_all['build_count_frame']\n",
    "del df_all['build_count_brick']\n",
    "del df_all['build_count_monolith']\n",
    "del df_all['build_count_panel']\n",
    "del df_all['build_count_foam']\n",
    "del df_all['build_count_slag']\n",
    "del df_all['build_count_mix']\n",
    "del df_all['raion_build_count_with_builddate_info']\n",
    "del df_all['build_count_before_1920']\n",
    "del df_all['build_count_1921-1945']\n",
    "del df_all['build_count_1946-1970']\n",
    "del df_all['build_count_1971-1995']\n",
    "del df_all['build_count_after_1995']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To fill NaN values with a specific number apply the function .fillna\n",
    "df_all['cafe_sum_500_min_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['cafe_sum_500_max_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['cafe_avg_price_500'].fillna(-99, inplace=True)\n",
    "df_all['cafe_sum_1000_min_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['cafe_sum_1000_max_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['metro_min_walk'].fillna(-99, inplace=True)\n",
    "df_all['metro_km_walk'].fillna(-99, inplace=True)\n",
    "df_all['railroad_station_walk_km'].fillna(-99, inplace=True)\n",
    "df_all['railroad_station_walk_min'].fillna(-99, inplace=True)\n",
    "df_all['ID_railroad_station_walk'].fillna(-99, inplace=True)\n",
    "df_all['prom_part_5000'].fillna(-99, inplace=True)\n",
    "df_all['cafe_sum_5000_min_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['cafe_sum_5000_max_price_avg'].fillna(-99, inplace=True)\n",
    "df_all['cafe_avg_price_5000'].fillna(-99, inplace=True)\n",
    "df_all['product_type'].fillna(-99, inplace=True)\n",
    "df_all['green_part_2000'].fillna(-99, inplace=True)\n",
    "df_all['kitch_proportions'].fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To fill the the mean use the .mean()\n",
    "df_all['cafe_avg_price_1000'].fillna(train['cafe_avg_price_1000'].mean(), inplace=True)\n",
    "df_all['cafe_sum_1500_min_price_avg'].fillna(train['cafe_sum_1500_min_price_avg'].mean(), inplace=True)\n",
    "df_all['cafe_sum_1500_max_price_avg'].fillna(train['cafe_sum_1500_max_price_avg'].mean(), inplace=True)\n",
    "df_all['cafe_avg_price_1500'].fillna(train['cafe_avg_price_1500'].mean(), inplace=True)\n",
    "df_all['cafe_sum_2000_min_price_avg'].fillna(train['cafe_sum_2000_min_price_avg'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To fill the the median use the .median()\n",
    "df_all['cafe_sum_2000_max_price_avg'].fillna(train['cafe_sum_2000_max_price_avg'].median(), inplace=True)\n",
    "df_all['cafe_avg_price_2000'].fillna(train['cafe_avg_price_2000'].median(), inplace=True)\n",
    "df_all['cafe_sum_3000_min_price_avg'].fillna(train['cafe_sum_3000_min_price_avg'].median(), inplace=True)\n",
    "df_all['cafe_sum_3000_max_price_avg'].fillna(train['cafe_sum_3000_max_price_avg'].median(), inplace=True)\n",
    "df_all['cafe_avg_price_3000'].fillna(train['cafe_avg_price_3000'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another big problem for ML Algorithms it's the representation of categorical variables.\n",
    "# this is because, most of this algoritms deals only with numeric representations. \n",
    "\n",
    "# Deal with categorical values\n",
    "df_numeric = df_all.select_dtypes(exclude=['object']) # Select columns with numerical variables\n",
    "\n",
    "df_obj = df_all.select_dtypes(include=['object']).copy() # Select columns with non numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df_obj:\n",
    "    df_obj[c] = pd.factorize(df_obj[c])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can join this two data frames using the function concat() \n",
    "df_all = pd.concat([df_numeric, df_obj], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a validation set\n",
    "#num_val = int(num_train * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After we cleasing our data in pandas, we need to transform this data in a numpy array\n",
    "# because the most popular machine learning packages only make the computation using this format\n",
    "# To to that, let's convert our dataframe \n",
    "X_all = df_all.values\n",
    "X = X_all[:num_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this step we'll use the train dataset to split in training and test to ensure that our algorithm \n",
    "# is learning and to perform some quality check over the rmse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) # We'll use 20% of the data for test\n",
    "\n",
    "print('X Train shape is', X_train.shape)\n",
    "print('Y Train shape is', y_train.shape)\n",
    "print('X Test shape is', X_test.shape)\n",
    "print('Y Test shape is', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear Regression models are models which fits the data in linear equations. Linear Regression models are really useful in many cases and it is, almost always, the first choice when dealing with a ML problem.\n",
    "\n",
    "There are some variants from the basic Linear Regression model. The two most common are Ridge Regression and Lasso Regression. The ideia behind these two models is regularize the model, eliminating useless features.\n",
    "\n",
    "References: \n",
    "\n",
    "- https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "- https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "- https://en.wikipedia.org/wiki/Linear_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll use the a Linear model to fit this data\n",
    "# First we use .LinearRegression() function of the linear_model library and make the object lm\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# After this, we make a object called model where we'll call the function .fit where we use as argument \n",
    "# X_train and Y_train datasets to perform the fit\n",
    "model = lm.fit(X_train, y_train)\n",
    "\n",
    "# In this step we'll make the predictions objects after we call the predict function using a \n",
    "# X_test dataframe as parameter \n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "# And now we perform a inverse transformation of the log using np.exp() function\n",
    "y_pred = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To make an assessment of the quality of the model, we'll use the RMSE as a main metric to assess the performance of\n",
    "# the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There's another way to do that\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmse_val = rmse(y_pred.astype('int64'),y_test)\n",
    "print(\"rms error is: \" + str(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's do some submission\n",
    "# First we'll get the records of test database and store in some object\n",
    "test_sub = X_all[:-num_train]\n",
    "print('Test Sub shape is', test_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll made the prediction using our test dataset and store inside y_pred object\n",
    "predictions = lm.predict(test_sub)\n",
    "\n",
    "y_pred = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submision\n",
    "\n",
    "You need to create your submission when you want to test your model using the test data. These next cells creates a file with the submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll join the id and the predictions and store in an object called df_submission\n",
    "df_submission = pd.DataFrame({'id': id_test, 'price_doc': y_pred.astype('int64')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what we got\n",
    "df_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To generate the file, we'll use to_csv function and we'll use the submission.csv ans the name of the file\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the submission format\n",
    "! head -n15 submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge (alpha = .5)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "predictions = reg.predict(X_test)\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha = 0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test)\n",
    "\n",
    "predictions = reg.predict(X_test)\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "This type of model creates a decision tree based on the parameters. Each node will have some specific constant for each one of the parameters. It is a very powerful model, combining the versatile of linear models with a more flexible from decision trees.\n",
    "\n",
    "References:\n",
    "- http://scikit-learn.org/stable/modules/tree.html#tree\n",
    "- https://en.wikipedia.org/wiki/Decision_tree_learning#Decision_tree_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient boosting is a technique which combines several weak models to predict the result. This kind of approach is named as ensamble. XGBoost is a library which implements the gradient boosting method. XGBoost is being used with very good results for Kaggle problems.\n",
    "\n",
    "\n",
    "References: \n",
    "- https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "- https://en.wikipedia.org/wiki/Xgboost\n",
    "- https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'reg:linear'\n",
    "params['eta'] = 0.02\n",
    "params['silent'] = 1\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "clf = xgb.train(params, d_train, 50, watchlist, early_stopping_rounds=100)\n",
    "\n",
    "predictions = clf.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "alpha = 0.95\n",
    "\n",
    "clf = GradientBoostingRegressor(loss='quantile', alpha=alpha,\n",
    "                                n_estimators=10, max_depth=3,\n",
    "                                learning_rate=.1, min_samples_leaf=9,\n",
    "                                min_samples_split=9)    \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalers\n",
    "\n",
    "It is very common to have very different scales for the features. This is, in general, very bad for almost all machine learning algorithms, it might cause, for example, numerical stability problems. In order to avoid these problems, SciKit Learn implements some scalaers.\n",
    "\n",
    "Refereces: \n",
    "- http://scikit-learn.org/stable/modules/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Today's hottest topic in ML world is Deep Learning. The basic idea of DL is use deep Neural Networks to learn a the data's pattern. The next cell shows how to create a very simple(not deep) in SciKitLearn. \n",
    "\n",
    "References: \n",
    " - http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(7, ), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "y_pred = np.exp(predictions)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred.astype('int64')))\n",
    "print(\"rms error is: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "\n",
    "Clustering algorithms cannot be used to solve this kind of problem, but they are very useful to discover more informations about the features and data. Here is a example on how to create some clustes for some points.\n",
    "\n",
    "References: \n",
    " - https://en.wikipedia.org/wiki/K-means_clustering\n",
    " - http://scikit-learn.org/stable/modules/clustering.html#k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X_train)\n",
    "kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General References:\n",
    "\n",
    "- http://scikit-learn.org/stable/user_guide.html\n",
    "- https://www.kaggle.com/ (You can find a lot of cool kernels here)\n",
    "- https://unsupervisedmethods.com/my-curated-list-of-ai-and-machine-learning-resources-from-around-the-web-9a97823b8524"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
